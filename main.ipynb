{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55860,) (55860,) (37240,)\n",
      "(55860,)\n",
      "(55860, 4208) (37240, 4208) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(55860, 4208) (37240, 4208)\n",
      "(55860, 4208) (37240, 4208) (55860,)\n",
      "(55860, 4208) (55860,)\n",
      "Epoch 1/20\n",
      "55860/55860 [==============================] - 86s - loss: 1.3042 - acc: 0.6545    \n",
      "Epoch 2/20\n",
      "55860/55860 [==============================] - 74s - loss: 0.6538 - acc: 0.8206    \n",
      "Epoch 3/20\n",
      "55860/55860 [==============================] - 74s - loss: 0.4513 - acc: 0.8763    \n",
      "Epoch 4/20\n",
      "55860/55860 [==============================] - 73s - loss: 0.3185 - acc: 0.9122    \n",
      "Epoch 5/20\n",
      "55860/55860 [==============================] - 75s - loss: 0.2222 - acc: 0.9375    \n",
      "Epoch 6/20\n",
      "55860/55860 [==============================] - 77s - loss: 0.1552 - acc: 0.9558    \n",
      "Epoch 7/20\n",
      "55860/55860 [==============================] - 77s - loss: 0.1141 - acc: 0.9670    \n",
      "Epoch 8/20\n",
      "55860/55860 [==============================] - 78s - loss: 0.0876 - acc: 0.9745    \n",
      "Epoch 9/20\n",
      "55860/55860 [==============================] - 78s - loss: 0.0702 - acc: 0.9789    \n",
      "Epoch 10/20\n",
      "55860/55860 [==============================] - 78s - loss: 0.0560 - acc: 0.9834    \n",
      "Epoch 11/20\n",
      "55860/55860 [==============================] - 78s - loss: 0.0508 - acc: 0.9843    \n",
      "Epoch 12/20\n",
      "55860/55860 [==============================] - 78s - loss: 0.0458 - acc: 0.9854    \n",
      "Epoch 13/20\n",
      "55860/55860 [==============================] - 78s - loss: 0.0410 - acc: 0.9875    \n",
      "Epoch 14/20\n",
      "55860/55860 [==============================] - 78s - loss: 0.0369 - acc: 0.9883    \n",
      "Epoch 15/20\n",
      "55860/55860 [==============================] - 78s - loss: 0.0340 - acc: 0.9893    \n",
      "Epoch 16/20\n",
      "55860/55860 [==============================] - 78s - loss: 0.0357 - acc: 0.9887    \n",
      "Epoch 17/20\n",
      "55860/55860 [==============================] - 78s - loss: 0.0336 - acc: 0.9889    \n",
      "Epoch 18/20\n",
      "55860/55860 [==============================] - 77s - loss: 0.0325 - acc: 0.9897    \n",
      "Epoch 19/20\n",
      "55860/55860 [==============================] - 77s - loss: 0.0273 - acc: 0.9911    \n",
      "Epoch 20/20\n",
      "55860/55860 [==============================] - 77s - loss: 0.0282 - acc: 0.9914    \n",
      "37240/37240 [==============================] - 24s    \n",
      "NN done\n"
     ]
    }
   ],
   "source": [
    "from string import ascii_letters\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy as sp\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import ensemble\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Reshape\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from time import clock\n",
    "from collections import defaultdict\n",
    "def toVector(x_train, x_test):\n",
    "    print x_train.shape\n",
    "    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 2))\n",
    "    # Vectorize the samples.\n",
    "    frames = [x_train, x_test]\n",
    "    result = pd.concat(frames)\n",
    "    ds = vectorizer.fit_transform(result)\n",
    "    #x_test = vectorizer.fit_transform(result)\n",
    "    #print x_train.shape, x_test.shape\n",
    "    xx_train = ds[:x_train.shape[0]][:]\n",
    "    xx_test = ds[x_train.shape[0]:][:]\n",
    "    print xx_train.shape, xx_test.shape, type(xx_train)\n",
    "    xx_train = pd.DataFrame(xx_train.todense())\n",
    "    xx_test = pd.DataFrame(xx_test.todense())\n",
    "    print xx_train.shape, xx_test.shape\n",
    "    return xx_train, xx_test\n",
    "    #return pd.DataFrame(eval(x_train)), pd.DataFrame(eval(x_test))\n",
    "def importData():\n",
    "    xy_train = pd.read_csv('evo_train.csv', delimiter=',', header=None)\n",
    "    print xy_train.shape\n",
    "    x_train = xy_train[:][0]\n",
    "    y_train = xy_train[:][1]\n",
    "    x_test = pd.read_csv('evo_test.csv', delimiter=',', header=None)[:][0]\n",
    "    print x_train.shape, y_train.shape, x_test.shape\n",
    "    y_tr1=open('y_tr1.csv', 'w')\n",
    "    for y in y_train:\n",
    "        y_tr1.write(str(y)+'\\n')\n",
    "    y_tr1.close()\n",
    "    return x_train, y_train, x_test\n",
    "def importData2():\n",
    "    x_train = pd.read_csv('x_tr.csv', delimiter=',', header=None)[:][0]\n",
    "    x_test = pd.read_csv('x_te.csv', delimiter=',', header=None)[:][0]\n",
    "    return x_train, x_test\n",
    "def convertToMin(x_train, x_test, n):\n",
    "    words = {}\n",
    "    for strW in x_train:\n",
    "        word1=strW.split(' ')\n",
    "        for word in word1:\n",
    "            if (not words.has_key(word)):\n",
    "                words[word]=1\n",
    "            else:\n",
    "                words[word]+=1\n",
    "    for strW in x_test:\n",
    "        word1=strW.split(' ')\n",
    "        for word in word1:\n",
    "            if (not words.has_key(word)):\n",
    "                words[word]=1\n",
    "            else:\n",
    "                words[word]+=1\n",
    "    x_tr=[]\n",
    "    i=1\n",
    "    for strW in x_train:\n",
    "        word1=strW.split(' ')\n",
    "        maxI=0\n",
    "        mmaxI=0\n",
    "        maxW=\"1\"\n",
    "        mmaxW=\"1\"\n",
    "        for word in word1:\n",
    "            if (len(word)>3):\n",
    "                #print len(word), (word), (words[word])\n",
    "                if (words[word]>maxI):\n",
    "                    mmaxW=maxW\n",
    "                    mmaxI=maxI\n",
    "                    maxW=word\n",
    "                    maxI=words[word]\n",
    "        x_tr.append(maxW+\" \"+mmaxW)\n",
    "        #x_tr.append(maxW)\n",
    "        i+=1\n",
    "        #print maxW\n",
    "        #if (i>100):\n",
    "        #    break\n",
    "    x_te=[]\n",
    "    for strW in x_test:\n",
    "        word1=strW.split(' ')\n",
    "        maxI=0\n",
    "        mmaxI=0\n",
    "        maxW=\"1\"\n",
    "        mmaxW=\"1\"\n",
    "        for word in word1:\n",
    "            if (len(word)>3):\n",
    "                #print len(word), (word), (words[word])\n",
    "                if (words[word]>maxI):\n",
    "                    mmaxW=maxW\n",
    "                    mmaxI=maxI\n",
    "                    maxW=word\n",
    "                    maxI=words[word]\n",
    "        x_te.append(maxW+\" \"+mmaxW)\n",
    "        #x_te.append(maxW)\n",
    "        i+=1\n",
    "        #print maxW\n",
    "        #if (i>100):\n",
    "        #    break\n",
    "    x_tr1=open('x_tr2.csv', 'w')\n",
    "    for x in x_tr:\n",
    "        x_tr1.write(str(x)+'\\n')\n",
    "    x_tr1.close()\n",
    "    x_te1=open('x_te2.csv', 'w')\n",
    "    for x in x_te:\n",
    "        x_te1.write(str(x)+'\\n')\n",
    "    x_te1.close()\n",
    "def importData3():\n",
    "    x_train = pd.read_csv('x_tr2.csv', delimiter=',', header=None)[:][0]\n",
    "    y_train = pd.read_csv('y_tr1.csv', delimiter=',', header=None)[:][0]\n",
    "    x_test = pd.read_csv('x_te2.csv', delimiter=',', header=None)[:][0]\n",
    "    return x_train, y_train, x_test\n",
    "def importDatans():\n",
    "    x_train = pd.read_csv('x_trns.csv', delimiter=',', header=None)[:][0]\n",
    "    y_train = pd.read_csv('y_tr1.csv', delimiter=',', header=None)[:][0]\n",
    "    x_test = pd.read_csv('x_tens.csv', delimiter=',', header=None)[:][0]\n",
    "    return x_train, y_train, x_test\n",
    "def convertDataset(x_tr, x_te):\n",
    "    for i in range (x_tr.shape[0]):\n",
    "        x_tr[i]=x_tr[i].decode('utf-8').lower()\n",
    "        newstr=[]\n",
    "        for j in range (len(x_tr[i])):\n",
    "            if (not (ord(x_tr[i][j]) in range(ord('0'), ord('9')+1) or ord(x_tr[i][j]) in range(ord(u'а'), ord(u'я')+1) or ord(x_tr[i][j]) in range(ord('a'), ord('z')+1) or x_tr[i][j]==' ')):\n",
    "                newstr.append(\" \")\n",
    "                continue\n",
    "            newstr.append(x_tr[i][j].decode('utf-8'))\n",
    "        newstr = ''.join(newstr)\n",
    "        newstr=re.sub(r'\\s+', '', newstr)\n",
    "        x_tr[i]=newstr\n",
    "    for i in range (x_te.shape[0]):\n",
    "        x_te[i]=x_te[i].decode('utf-8').lower()\n",
    "        newstr=[]\n",
    "        for j in range (len(x_te[i])):\n",
    "            if (not (ord(x_te[i][j]) in range(ord('0'), ord('9')+1) or ord(x_te[i][j]) in range(ord(u'а'), ord(u'я')+1) or ord(x_te[i][j]) in range(ord('a'), ord('z')+1) or x_te[i][j]==' ')):\n",
    "                newstr.append(\" \")\n",
    "                continue\n",
    "            newstr.append(x_te[i][j].decode('utf-8'))\n",
    "        newstr = ''.join(newstr)\n",
    "        newstr=re.sub(r'\\s+', '', newstr)\n",
    "        x_te[i]=newstr\n",
    "    x_tr1=open('x_trns.csv', 'w')\n",
    "    for x in x_tr:\n",
    "        x_tr1.write(str(x)+'\\n')\n",
    "    x_tr1.close()\n",
    "    x_te1=open('x_tens.csv', 'w')\n",
    "    for x in x_te:\n",
    "        x_te1.write(str(x)+'\\n')\n",
    "    x_te1.close()\n",
    "    return x_tr, x_te\n",
    "def LSTMRes(X_train, X_test, y_train):\n",
    "    y_train = y_train.as_matrix()\n",
    "    X_train = X_train.as_matrix()\n",
    "    X_test = X_test.as_matrix()\n",
    "    print X_train.shape, X_test.shape, y_train.shape\n",
    "    print X_train.shape, y_train.shape\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(61, activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=128)  \n",
    "    zLSMT = model.predict_classes(X_test)\n",
    "    y_res=open('y_testLSTM.csv', 'w')\n",
    "    for i in range (len(zLSMT)):\n",
    "        y_res.write(str(i)+\",\"+str(zLSMT[i])+\"\\n\")\n",
    "    y_res.close()\n",
    "    print \"NN done\"\n",
    "    return zLSMT\n",
    "def xgboostRes(X_train, X_test, y_train):\n",
    "    param = {'max_depth':4, 'silent':1, 'objective':'binary:logistic'}\n",
    "    bst = XGBClassifier(**param)\n",
    "    print X_train.shape, y_train.shape\n",
    "    bst.fit(X_train, y_train)\n",
    "    print \"fit done\"\n",
    "    zG = bst.predict(X_test)\n",
    "    y_res=open('y_testXGB.csv', 'w')\n",
    "    #y_res.write('\\n'.join(str(v) for v in zG))   \n",
    "    y_res.write(\"id,GROUP_ID\\n\")\n",
    "    for i in range (len(zG)):\n",
    "    #for i in range (37240):\n",
    "        #y_res.write(str(i)+\",14\\n\")\n",
    "        y_res.write(str(i)+\",\"+str(zG[i])+\"\\n\")\n",
    "    y_res.close()\n",
    "    print \"xgb done\"\n",
    "def main():\n",
    "    nnnn=5000\n",
    "    #x_train, y_train, x_test = importData()\n",
    "    #x_train, x_test = convertDataset(x_train, x_test)\n",
    "    #x_train, x_test = importData2()\n",
    "    #convertToMin(x_train, x_test, 1)\n",
    "    #x_train, y_train, x_test = importData3()\n",
    "    x_train, y_train, x_test = importDatans()\n",
    "    print x_train.shape, y_train.shape, x_test.shape\n",
    "    x_train, x_test = toVector(x_train[:][:], x_test[:][:])\n",
    "    #x_train, x_test = toVector(x_train[:nnnn][:], x_test[:nnnn][:])\n",
    "    #LSTMRes(x_train, x_test, y_train[:nnnn])\n",
    "    LSTMRes(x_train, x_test, y_train[:])\n",
    "    #print \"xg\"\n",
    "    #st = clock()\n",
    "    #xgboostRes(x_train[:nnnn][:], x_test[:nnnn][:], y_train[:nnnn])\n",
    "    #print clock()-st\n",
    "    #xgboostRes(x_train[:][:], x_test[:][:], y_train[:])\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
